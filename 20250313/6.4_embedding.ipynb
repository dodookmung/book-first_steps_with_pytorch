{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 임베딩 함수를 이용한 LSTM, GRU 모델 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Recurrent Neural Network\n",
    "- 셰익스피어 문체를 모방하는 순환신경망 실습 코드입니다.\n",
    "- Embedding 레이어 및 RNN 모델로 구성되어 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import time, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
      "\u000b\f\n",
      "num_chars =  100\n"
     ]
    }
   ],
   "source": [
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "print(all_characters)\n",
    "print('num_chars = ', n_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-13 12:00:26--  https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8001::154, 2606:50c0:8003::154, 2606:50c0:8000::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8001::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘./data/input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  2.70MB/s    in 0.4s    \n",
      "\n",
      "2025-03-13 12:00:27 (2.70 MB/s) - ‘./data/input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/tinyshakespeare/input.txt -P ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 1115394\n"
     ]
    }
   ],
   "source": [
    "file = unidecode.unidecode(open('./data/input.txt').read())\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2000\n",
    "print_every = 100\n",
    "plot_every = 10\n",
    "\n",
    "# chunk에 대한 설명은 아래 함수정의하면서 하겠습니다.\n",
    "chunk_len = 200\n",
    "\n",
    "hidden_size = 100\n",
    "batch_size = 1\n",
    "num_layers = 1\n",
    "embedding_size = 70\n",
    "lr = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_check():\n",
    "    start_index = random.randint(0, file_len - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return file[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([36, 37, 38, 13, 14, 15])\n"
     ]
    }
   ],
   "source": [
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string), dtype=torch.long)\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return tensor\n",
    "\n",
    "print(char_tensor('ABCdef'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_training_set():\n",
    "    chunk = random_check()\n",
    "    inp = char_tensor(chunk[:-1])\n",
    "    target = char_tensor(chunk[1:])\n",
    "    return inp, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        self.encoder = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.RNN(embedding_size, hidden_size, num_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        out = self.encoder(input.view(1, -1))\n",
    "        out, hidden = self.rnn(out, hidden)\n",
    "        out = self.decoder(out.view(batch_size, -1))\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, hidden_size)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(input_size=n_characters,\n",
    "            embedding_size=embedding_size,\n",
    "            hidden_size=hidden_size,\n",
    "            output_size=n_characters,\n",
    "            num_layers=2\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = char_tensor(\"A\")\n",
    "hidden = model.init_hidden()\n",
    "out, hidden = model(inp, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임의의 문자(start_str)로 시작하는 길이 200짜리 모방 글을 생성하는 코드입니다.\n",
    "def test():\n",
    "    start_str = \"b\"\n",
    "    inp = char_tensor(start_str)\n",
    "    hidden = model.init_hidden()\n",
    "    x = inp\n",
    "\n",
    "    print(start_str,end=\"\")\n",
    "    for i in range(200):\n",
    "        output,hidden = model(x,hidden)\n",
    "\n",
    "        # 여기서 max값을 사용하지 않고 multinomial을 사용하는 이유는 만약 max 값만 쓰는 경우에\n",
    "        # 생성되는 텍스트가 다 the the the the the 이런식으로 나오기 때문입니다.\n",
    "        # multinomial 함수를 통해 높은 값을 가지는 문자들중에 램덤하게 다음 글자를 뽑아내는 방식으로 자연스러운 텍스트를 생성해냅니다.\n",
    "        output_dist = output.data.view(-1).div(0.8).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        predicted_char = all_characters[top_i]\n",
    "\n",
    "        print(predicted_char,end=\"\")\n",
    "\n",
    "        x = char_tensor(predicted_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " tensor([4.6174], grad_fn=<DivBackward0>) \n",
      "\n",
      "mg+%!3$qAX|Wc(\u000b\"n^wdEj+aGH@x( o~H/JP\fsc)(FV,!pLxIu$5J\n",
      "B)lRF=VtLfHjF~Rz'Ag&Eo/t,\\3V>o0YONj3n\f4*]]2$4$~?;1$Ro7ScS>KC-Vt:]GB\"BWcMH-%\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.4498], grad_fn=<DivBackward0>) \n",
      "\n",
      "bit on for.\n",
      "\n",
      "LUwans, ming sor the mausg, ani ofe ur waralo thare yon cor lr.\n",
      "af fit and wnwoy hy hir as antes thon wisg ir thar rortre wous, cile;\n",
      "ondthe or athes heer sinaring \n",
      "Tore ar mas hive fass f\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.3921], grad_fn=<DivBackward0>) \n",
      "\n",
      "b, dech futhece porpy imde wrete, sont cIA:\n",
      "Thy miet pith shind the hers, bage hat are iler nouest mey;\n",
      "Whath mowea! wering beverisheldey hoy stall, you brome ghlest she wuree, sife, the nito morenger \n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.0533], grad_fn=<DivBackward0>) \n",
      "\n",
      "be if that, wish he you not wine ar? and on to to lill deed you wousmin& trave te to moud, the my to lave thins mise my he sery the we hapings dalt o thich araty to thing onds ou and me my many,\n",
      "-comin\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.1013], grad_fn=<DivBackward0>) \n",
      "\n",
      "brofss.\n",
      "\n",
      "TIIUCHOR:\n",
      "Sece well miegeter,\n",
      "then.\n",
      "\n",
      "SRTHThy have soress all wicesuine bno I ford the ce uliersepe wrive rol seamy mere undedelted not not lemintere.\n",
      "\n",
      "GLIIR:\n",
      "Sead wit.\n",
      "\n",
      "Thin to fhiven parvire,\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.2388], grad_fn=<DivBackward0>) \n",
      "\n",
      "be rove purth me mited theur'd think you for mees jorm hand mates fall tham bems our stay wace at earf'n.\n",
      "Whount adture thee say, thing ware did sronsin.\n",
      "\n",
      "IITES:\n",
      "Som\n",
      "juo ase we and that they frimones f\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.0683], grad_fn=<DivBackward0>) \n",
      "\n",
      "bleckis monstade, ferment mecour mevered andeland uemer, nith\n",
      "To stuenbist your and houself\n",
      "OMWRom a marf'll,\n",
      "But cholde,\n",
      "Werbeen\n",
      "soy ponguent's!\n",
      "No goin, and will benou\n",
      "\n",
      "LUCMETRASTINGI:\n",
      "Faise monelero\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.0048], grad_fn=<DivBackward0>) \n",
      "\n",
      "blion,\n",
      "But of the her of for bith suens':\n",
      "A hear op they comuld bith, vorde he whiriher thon when and my horsssed and my estouck an leasinstert mest onoth be over.\n",
      "\n",
      "BROUCHIS:\n",
      "Now have must the so the a\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8026], grad_fn=<DivBackward0>) \n",
      "\n",
      "by shear and haeself you kare mort?\n",
      "Thou mum the chilk be and to lices,\n",
      "And if your the tond in the maeces:\n",
      "Ast thy wely deats to he and thou not you dost thithe then, and and that, their me, beser pro\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.9474], grad_fn=<DivBackward0>) \n",
      "\n",
      "by congal sclood word.\n",
      "\n",
      "MEGY IIEN:\n",
      "Thet I Habe.\n",
      "\n",
      "MERWICGART Nor have but dettat with mus, afford, it your fears\n",
      "The sid concay and your to what weis hall go tiwer of Mur cay whis the fiet:\n",
      "No mand\n",
      "Efar\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.9493], grad_fn=<DivBackward0>) \n",
      "\n",
      "beth gecehple sold hame, infore mave,\n",
      "Pongeis more or with if the sim we eyer, I, poats, have to me lain with what, soce'n to tell mar.\n",
      "\n",
      "HARD\n",
      "GONCESTER SEUREN ETRY:\n",
      "Thom suve endiast what lean deap of \n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8002], grad_fn=<DivBackward0>) \n",
      "\n",
      "bate anjore abone, seats and conto one dien for by that dees, our whight loth my lay,\n",
      "And a grience, that our pogee.\n",
      "\n",
      "LECHIO:\n",
      "That recane fleed porce not in the onether,\n",
      "I shall that to to lone\n",
      "Me tull\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.9724], grad_fn=<DivBackward0>) \n",
      "\n",
      "bed as when himest, let one and chall, ore helpet,\n",
      "The bring sussed frow mace at him father your know nor seise the hand think them now-lorest trouther povather--oncenne, eown chathing woolly a fores: \n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.6806], grad_fn=<DivBackward0>) \n",
      "\n",
      "becciling youss\n",
      "Martue selter'd good, I bake, I feited wing.\n",
      "\n",
      "WICHARD:\n",
      "Herought.\n",
      "\n",
      "KANVE IV:\n",
      "Now the being,\n",
      "Hompen centeas?\n",
      "Det the hands,\n",
      "Thing bearn of you? nectarting the would searn, speast,\n",
      "Who thi\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.9962], grad_fn=<DivBackward0>) \n",
      "\n",
      "bose, not epeess our dull remal.\n",
      "\n",
      "KING RUSEL, sture, brother in and with the blird, by one latte,\n",
      "Would at the her and thing let tell my he did\n",
      "Fill taur's the pruest lagroth of that uncorder, in diefy\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.7883], grad_fn=<DivBackward0>) \n",
      "\n",
      "by Marks is I have uponn:\n",
      "As the trow the far?\n",
      "\n",
      "DUKENENAUS:\n",
      "What love aghorgers bear with the com't is kiltiritinctle had\n",
      "Thimster;\n",
      "Which be parts mish.\n",
      "\n",
      "MARGALE:\n",
      "To repood his order a what sqailiend,\n",
      "\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.7704], grad_fn=<DivBackward0>) \n",
      "\n",
      "ble have wars me; heart\n",
      "I'll amsqucenw-domess of thou an, now have hery why, the for the epon your so?\n",
      "\n",
      "Secoy to know lies, him, such a for again; and\n",
      "I no his will,\n",
      "Which you tay and will havy bood up\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8165], grad_fn=<DivBackward0>) \n",
      "\n",
      "bes,\n",
      "And be the\n",
      "Lones. What bear you tentious you seave.\n",
      "3 stidly trouk.\n",
      "\n",
      "Th RINCELIO:\n",
      "My with, teart\n",
      "'to the been me pertore hands death, comeding would heart is with of thy deaded at think that sone?\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.0570], grad_fn=<DivBackward0>) \n",
      "\n",
      "beashing then you sure haw your salut frue!\n",
      "\n",
      "PETRUCHIANIO:\n",
      "The good good from the confed go men my couse ou say hereer misters, and as will the heards and unted the come,\n",
      "They like our conseing his bro\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8952], grad_fn=<DivBackward0>) \n",
      "\n",
      "biest of onress our was sake.\n",
      "\n",
      "TORWIOD:\n",
      "Am poor of thou shall your for world to omb, and and to be heart the strifeddite, houther, this mosures of the Elands.\n",
      "\n",
      "GREMIO:\n",
      "Go, but for this? as you shand ha\n",
      " ====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_epochs):\n",
    "    total = char_tensor(random_check())\n",
    "    inp = total[:-1]\n",
    "    label = total[1:]\n",
    "    hidden = model.init_hidden()\n",
    "\n",
    "    loss = torch.tensor([0]).type(torch.FloatTensor)\n",
    "    optimizer.zero_grad()\n",
    "    for j in range(chunk_len - 1):\n",
    "        x = inp[j]\n",
    "        y_ = label[j].unsqueeze(0).type(torch.LongTensor)\n",
    "        y, hidden = model(x, hidden)\n",
    "        loss += loss_func(y, y_)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"\\n\",loss/chunk_len,\"\\n\")\n",
    "        test()\n",
    "        print(\"\\n\",\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers=1):\n",
    "        super(GRU, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        self.encoder = nn.Embedding(input_size, embedding_size)\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, num_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        out = self.encoder(input.view(1, -1))\n",
    "        out, hidden = self.gru(out, hidden)\n",
    "        out = self.decoder(out.view(batch_size, -1))\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, hidden_size)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru = GRU(input_size=n_characters,\n",
    "            embedding_size=embedding_size,\n",
    "            hidden_size=hidden_size,\n",
    "            output_size=n_characters,\n",
    "            num_layers=2\n",
    "            )\n",
    "\n",
    "inp = char_tensor(\"A\")\n",
    "hidden = model.init_hidden()\n",
    "out, hidden = model(inp, hidden)\n",
    "\n",
    "optimizer = torch.optim.Adam(model_gru.parameters(), lr=lr)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " tensor([4.5872], grad_fn=<DivBackward0>) \n",
      "\n",
      "by and God what;\n",
      "Gold to shall persain tear, it is any what hours.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "Is toints stainssering the was fried decomeended; by not that carition of that;\n",
      "If sill no that had Tare Jomkers her\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.3812], grad_fn=<DivBackward0>) \n",
      "\n",
      "bat,\n",
      "This are is be sent had condect undeay?\n",
      "\n",
      "DUKE OF YORK:\n",
      "His duched a tright of starratiinstaite, for deash.\n",
      "\n",
      "CLARAD\n",
      "LEROT:\n",
      "I'll me expirent the manfer a griest our sober\n",
      "To may the shall him in a f\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.2237], grad_fn=<DivBackward0>) \n",
      "\n",
      "bling is with\n",
      "For end nase not hasts her sparitor and see little her, a kilk; and saw, and the seed, and be struer,\n",
      "Yet strects, made for of come contury in for the delision,\n",
      "That them but may the good\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.2607], grad_fn=<DivBackward0>) \n",
      "\n",
      "by follow and for conforss,\n",
      "Time that counded, to him, I am lay seeked will preist\n",
      "And with sight, here a for, my life, lame?\n",
      "\n",
      "CALT:\n",
      "Which need somp? so, too them brise, what steek at stright\n",
      "Than stay\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.1583], grad_fn=<DivBackward0>) \n",
      "\n",
      "bupt these a seed;\n",
      "And, the shall in the prest is thou have presust liep he dow on eyes slain any stendss\n",
      "ere staxely with that, I praint and this as shall are on?\n",
      "\n",
      "GRUCOSHA:\n",
      "Oy hours brothest comepon \n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.2856], grad_fn=<DivBackward0>) \n",
      "\n",
      "borritt; he proint gaint,\n",
      "There; I prieven'gs they power as the diegh and of it his in thee;\n",
      "The death suy the sleet sunberd the havely heart sint their caring\n",
      "I may, that I would ignry is it eeep shal\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.1889], grad_fn=<DivBackward0>) \n",
      "\n",
      "bure suple which hast face thy such I say not and Egrear's live as lives.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "And it drown swere's sill, grast:\n",
      "There the swearess us; sir, and is with come, sunters: when Antent,\n",
      "Miled, u\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.7976], grad_fn=<DivBackward0>) \n",
      "\n",
      "breother;\n",
      "If they like your not it do, I wife, but but son, and so sint, and were before I did it but what compatts to joims:\n",
      "Eites and very some our comead, brothed how lives it the mastest so.\n",
      "I not \n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8180], grad_fn=<DivBackward0>) \n",
      "\n",
      "by:\n",
      "My kind not bibre, live a cale's in is tire of here:\n",
      "And speet:\n",
      "Who died last age restrass the do a\n",
      "less not thy's lied is word,\n",
      "There with a stand the grather and make how\n",
      "But be pardess to you ou\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.0700], grad_fn=<DivBackward0>) \n",
      "\n",
      "by use,\n",
      "You suat you, saish a low ladister's eign\n",
      "broak an undern wife his shall boy,\n",
      "Them my daitie. and prow of the sumaition nis and\n",
      "I have is with had in thee,\n",
      "And the grued Rome the was wutition,\n",
      "\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8245], grad_fn=<DivBackward0>) \n",
      "\n",
      "by in the vesest,\n",
      "Whilk, the canst a scarant but the lown thee.\n",
      "\n",
      "QUEEN RURHAN:\n",
      "Go an his all burked death it:\n",
      "I am that fathed bed word.\n",
      "\n",
      "BALTANCESTE:\n",
      "But thou that my squeenful thee this sund of marve\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.7190], grad_fn=<DivBackward0>) \n",
      "\n",
      "brong and steed,\n",
      "That 'to me come some they love: Oxfore that chase me no not take in the look; you spottuit we is she congreat an with a will commant your tetuit,\n",
      "And still there, store was is thee I \n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.7979], grad_fn=<DivBackward0>) \n",
      "\n",
      "bable again:\n",
      "The king the skill of thee shy and our thing see.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "I they is now shald is that sufful too, son us the is compenterst;\n",
      "You and my that his hear for out you piritor of is tha\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8187], grad_fn=<DivBackward0>) \n",
      "\n",
      "blost exenish agy;\n",
      "Be grow on them, let in hast sut cursed in thou bast\n",
      "Thill deaten her death; I shure, and chains?\n",
      "\n",
      "BALTAS:\n",
      "You shall withe to a party, if the spent is them of it, and I send; God sel\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.6392], grad_fn=<DivBackward0>) \n",
      "\n",
      "bred see, what sid,\n",
      "That were is kney, will he shall stand be thought\n",
      "The down, coust up, and eise and creas, with a liege.\n",
      "\n",
      "POLPESBER:\n",
      "Oxford I shall I me, if what that who---\n",
      "The still he sut thussel\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8300], grad_fn=<DivBackward0>) \n",
      "\n",
      "bord furmest:\n",
      "Be a care coust, if you our forset confers, and croward;\n",
      "What seek, thou call but my lord to thy diant?\n",
      "\n",
      "CORIOLIET:\n",
      "The seet see subpe be a king.\n",
      "\n",
      "BUCKING ETHY:\n",
      "I shall your light you dea\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8233], grad_fn=<DivBackward0>) \n",
      "\n",
      "ble is the losformied and for, but, that and take\n",
      "To call, genties a thanks the comied;\n",
      "And is for has ever have in had fried like busince.\n",
      "\n",
      "GLOUTESBOLY:\n",
      "This istire in a trown, if God wond Cale and fr\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.6697], grad_fn=<DivBackward0>) \n",
      "\n",
      "beling a confath,\n",
      "To his arrioo, sir, and fair, they led to shich day, what on the father:\n",
      "There; who condection you and live is an any sue will that live; I learful and do to my lord.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8047], grad_fn=<DivBackward0>) \n",
      "\n",
      "bito, have give not me so:\n",
      "Of my let resices gift of it deast,\n",
      "What make a will such affie,\n",
      "God remans an the same of thee, death he shore them say, why, with and son,\n",
      "Thou shall sigrow, in distait, I \n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.4807], grad_fn=<DivBackward0>) \n",
      "\n",
      "by and low'd in the drair not wy rathery:\n",
      "To comes sights, come, who then, for my streodic,\n",
      "I dousines is sicby, go ding thee detless iless.\n",
      "\n",
      "CLAREND:\n",
      "If should or Lewared Pettend and these is relear;\n",
      "\n",
      " ====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_epochs):\n",
    "    total = char_tensor(random_check())\n",
    "    inp = total[:-1]\n",
    "    label = total[1:]\n",
    "    hidden = model_gru.init_hidden()\n",
    "\n",
    "    loss = torch.tensor([0]).type(torch.FloatTensor)\n",
    "    optimizer.zero_grad()\n",
    "    for j in range(chunk_len - 1):\n",
    "        x = inp[j]\n",
    "        y_ = label[j].unsqueeze(0).type(torch.LongTensor)\n",
    "        y, hidden = model_gru(x, hidden)\n",
    "        loss += loss_func(y, y_)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"\\n\",loss/chunk_len,\"\\n\")\n",
    "        test()\n",
    "        print(\"\\n\",\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        self.encoder = nn.Embedding(input_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        out = self.encoder(input.view(1, -1))\n",
    "        out, (hidden, cell) = self.lstm(out, (hidden, cell))\n",
    "        out = self.decoder(out.view(batch_size, -1))\n",
    "        return out, hidden, cell\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, hidden_size)\n",
    "        cell = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = LSTM(n_characters, embedding_size, hidden_size, n_characters, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([36])\n",
      "torch.Size([1, 1, 100])\n",
      "torch.Size([1, 100])\n"
     ]
    }
   ],
   "source": [
    "inp = char_tensor(\"A\")\n",
    "print(inp)\n",
    "hidden,cell = model_lstm.init_hidden()\n",
    "print(hidden.size())\n",
    "\n",
    "out,hidden,cell = model_lstm(inp,hidden,cell)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model_lstm.parameters(), lr=lr)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " tensor([4.5843], grad_fn=<DivBackward0>) \n",
      "\n",
      "band you are is down,\n",
      "The waty the deifenty thou truther: is crow lawt,-budes reaaat cut droise, she had!\n",
      "What the parant and besistuouses; a hadging,\n",
      "That was is my same and in but thee to thee life,\n",
      "\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.8146], grad_fn=<DivBackward0>) \n",
      "\n",
      "by: where long canless\n",
      "They froath a love of the kind for\n",
      "Caice; sholate word, there, but sir, be crown:\n",
      "If clast pease the speak, is it doo how them doth at say,\n",
      "Come to be speaks countonds to ague a \n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.2264], grad_fn=<DivBackward0>) \n",
      "\n",
      "bain the prisure a suffed of a hend\n",
      "Theale and stay the sunss, no shall you? Carspast are; or thou care, and sore.\n",
      "Do that ware if the will down they she no last the name and me\n",
      "the sumby's his steet I\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.3018], grad_fn=<DivBackward0>) \n",
      "\n",
      "by: whild him:\n",
      "This pieful to you some, serves: came are the heart:\n",
      "This as that the shall, that this not reseek'st the shall sace, to do an good me death.\n",
      "\n",
      "Sroive;\n",
      "That how cost there one, so go the s\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.3348], grad_fn=<DivBackward0>) \n",
      "\n",
      "badd,\n",
      "All peace when silloo cares the concress allight!\n",
      "Which viigning with you go this grain: I o'stay so;\n",
      "Go know come unted, seef, post you hast, good,\n",
      "Rearfess time fair mose to comid,\n",
      "Who a suchs,\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.1935], grad_fn=<DivBackward0>) \n",
      "\n",
      "blost being and his with,\n",
      "In wilk lessuride stilven an can with latter,\n",
      "And man is revere nest seekes, the death-ding; what thear his for like say.\n",
      "\n",
      "PETRUCHIO:\n",
      "Why! What death of if a dispelition,\n",
      "Of t\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.0573], grad_fn=<DivBackward0>) \n",
      "\n",
      "basten for the doth\n",
      "This the scrown for a falleator use, and you, sick or in there's wardy,\n",
      "And stie; have of shall the will lay, good Boit, as of shall,\n",
      "I soul still gone, for no but core, ond that I \n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.1209], grad_fn=<DivBackward0>) \n",
      "\n",
      "blaantes up!\n",
      "But your'd that light us. O for I for to conks!\n",
      "\n",
      "CARLIXEND:\n",
      "I would, supt stilo! in that dooss; your purumious hose.\n",
      "\n",
      "BALF\n",
      "And son, this thy spents and my foil end colder;\n",
      "The spance to a \n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.0074], grad_fn=<DivBackward0>) \n",
      "\n",
      "blay ward;\n",
      "He stablo, if Rome, with this of Lood to charmied did lord.\n",
      "\n",
      "VOLUMNANENO:\n",
      "King may have say, you man. Liaritet,\n",
      "Dow then agaves sweet you stoll the word this slay and to so grance,\n",
      "Sempy me \n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.1675], grad_fn=<DivBackward0>) \n",
      "\n",
      "by, or longs, if it grave back, such thee; where to theme is winks\n",
      "The vivile in to so pace?\n",
      "\n",
      "BOLLHAR LAURENLAS:\n",
      "I would is be dear and suister in time to plaint,\n",
      "in they friend that the sho, in supper\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.9862], grad_fn=<DivBackward0>) \n",
      "\n",
      "by; for Lord, that doth;\n",
      "Thy doing entunt say, and is my sorrow.\n",
      "\n",
      "PRINHIO:\n",
      "But you hand of life to he the learing gow,\n",
      "Pist is your hail that 'Tay; this though con; live in his say onsy praber the fare\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.1219], grad_fn=<DivBackward0>) \n",
      "\n",
      "bould have mights subpond to that.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Your scare forthing do doth, or death is in thee\n",
      "And coming.\n",
      "\n",
      "GRUCIO:\n",
      "The cairst my paten'st sleen aloove is die for and swere curours,\n",
      "Bestremisulvi\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.7680], grad_fn=<DivBackward0>) \n",
      "\n",
      "ble put to then wision,\n",
      "Than proyons she shateth with come,\n",
      "Where is theret, thus give of God, and deep whend thoughtess,\n",
      "which hese think them an he for you such is no sukes of they, bring,\n",
      "If prestt,\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.2312], grad_fn=<DivBackward0>) \n",
      "\n",
      "but way, if that is the sue, if their faber resere, on morst thee the mowpous\n",
      "I parch from if love my hath is to myself is whirst wherife,\n",
      "That thou life is distare speet it sworr's and the ware,\n",
      "In th\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.1104], grad_fn=<DivBackward0>) \n",
      "\n",
      "bushands subjeed and such of his.\n",
      "\n",
      "JULIET BENTIO:\n",
      "Good not for to deat theres briss, comford the refervain's fare:\n",
      "Is ristint is not but that such to thant thy fater may deed?\n",
      "\n",
      "BULT:\n",
      "I Herst Mlammant s\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.6719], grad_fn=<DivBackward0>) \n",
      "\n",
      "batled your loid?\n",
      "\n",
      "Hoss,\n",
      "Is your good see a with you, and thou, my necked.\n",
      "\n",
      "DUKE VOSTER:\n",
      "If thee, come and by thou doness a sucht in the care moncuss a for days;\n",
      "But a wilk you liver to so doth and tan\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.0878], grad_fn=<DivBackward0>) \n",
      "\n",
      "blist to miss starver\n",
      "What meet can much refors a day, and a stares;\n",
      "That sinly have what the some, mosinst,---yet so stainst where gold obstress;\n",
      "And say the walchough aft when with thee ade the death\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8984], grad_fn=<DivBackward0>) \n",
      "\n",
      "blow law\n",
      "As thy day, by theorief of my wards,\n",
      "Is not no the such sours so pring.\n",
      "\n",
      "BUCLAUD:\n",
      "Ior burther steet with that not mind!\n",
      "\n",
      "GLOUCESTER:\n",
      "Of the caubpt to make with hand what we subpers where is ca\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.0075], grad_fn=<DivBackward0>) \n",
      "\n",
      "bless, will day her treess\n",
      "My wilf is, that hand in taies up and bonour,\n",
      "Ward is this a has me post greatost in steeble\n",
      "And is our larded a slews a strievar;\n",
      "A crown, sir voars it be is ray death.\n",
      "\n",
      "LIS\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8081], grad_fn=<DivBackward0>) \n",
      "\n",
      "bond ears giving.\n",
      "\n",
      "BU\n",
      "SONTER:\n",
      "That is resigunted eyes of let this dog how serve, come, if Shack,\n",
      "That you from to royour, to see swere clast the king\n",
      "The kings, in time, I shall destritust sunk now, an\n",
      " ====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_epochs):\n",
    "    # 랜덤한 텍스트 덩어리를 샘플링하고 이를 인덱스 텐서로 변환합니다. \n",
    "    inp,label = random_training_set()\n",
    "    hidden, cell = model_lstm.init_hidden()\n",
    "\n",
    "    loss = torch.tensor([0]).type(torch.FloatTensor)\n",
    "    optimizer.zero_grad()\n",
    "    for j in range(chunk_len-1):\n",
    "        x  = inp[j]\n",
    "        y_ = label[j].unsqueeze(0).type(torch.LongTensor)\n",
    "        y,hidden,cell = model_lstm(x, hidden, cell)\n",
    "        loss += loss_func(y,y_)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"\\n\",loss/chunk_len,\"\\n\")\n",
    "        test()\n",
    "        print(\"\\n\",\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
